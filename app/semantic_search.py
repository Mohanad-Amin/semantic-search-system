"""
Ù…Ø­Ø±Ùƒ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
ÙŠØ¯ÙŠØ± ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØªÙ†ÙÙŠØ° Ø§Ù„Ø¨Ø­Ø«
"""

import os
import asyncio
import logging
import numpy as np
import pandas as pd
from pathlib import Path
from typing import List, Dict, Optional, Any
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

logger = logging.getLogger(__name__)

class SemanticSearchEngine:
    """Ù…Ø­Ø±Ùƒ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
    
    def __init__(self, model_path: str, data_path: str, embeddings_path: str):
        self.model_path = Path(model_path)
        self.data_path = Path(data_path)
        self.embeddings_path = Path(embeddings_path)
        
        # Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
        self.model: Optional[SentenceTransformer] = None
        self.knowledge_base_df: Optional[pd.DataFrame] = None
        self.embeddings: Optional[np.ndarray] = None
        self._is_ready = False
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self.search_count = 0
        self.total_processing_time = 0.0
        
    async def initialize(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨Ø§Ù„ÙƒØ§Ù…Ù„"""
        try:
            logger.info("ðŸ”„ Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„Ø¨Ø­Ø«...")
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            await self._load_model()
            
            # ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©
            await self._load_knowledge_base()
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù€ embeddings
            await self._load_embeddings()
            
            self._is_ready = True
            logger.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„Ø¨Ø­Ø« Ø¨Ù†Ø¬Ø§Ø­!")
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„Ø¨Ø­Ø«: {str(e)}")
            raise
    
    async def _load_model(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨"""
        try:
            logger.info(f"ðŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù†: {self.model_path}")
            
            if not self.model_path.exists():
                raise FileNotFoundError(f"Ù…Ø³Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {self.model_path}")
            
            # ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
            required_files = [
                "config.json",
                "model.safetensors",
                "tokenizer.json",
                "sentence_bert_config.json"
            ]
            
            missing_files = []
            for file in required_files:
                if not (self.model_path / file).exists():
                    missing_files.append(file)
            
            if missing_files:
                raise FileNotFoundError(f"Ù…Ù„ÙØ§Øª Ù…Ø·Ù„ÙˆØ¨Ø© Ù…ÙÙ‚ÙˆØ¯Ø©: {missing_files}")
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            self.model = SentenceTransformer(str(self.model_path))
            logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­!")
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {str(e)}")
            # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ ÙƒØ¨Ø¯ÙŠÙ„
            try:
                logger.info("ðŸ”„ Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ...")
                self.model = SentenceTransformer("intfloat/multilingual-e5-large")
                logger.warning("âš ï¸ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ù…Ø¯Ø±Ø¨")
            except Exception as fallback_error:
                logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: {str(fallback_error)}")
                raise
    
    async def _load_knowledge_base(self):
        """ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©"""
        try:
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ù„Ù Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            data_files = list(self.data_path.glob("*.xlsx"))
            if not data_files:
                raise FileNotFoundError(f"Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„ÙØ§Øª Excel ÙÙŠ: {self.data_path}")
            
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙˆÙ„ Ù…Ù„Ù Ù…ÙˆØ¬ÙˆØ¯
            data_file = data_files[0]
            logger.info(f"ðŸ“š ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù…Ù†: {data_file}")
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            self.knowledge_base_df = pd.read_excel(data_file, dtype=str)
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
            required_columns = ["id", "text"]
            missing_columns = [col for col in required_columns if col not in self.knowledge_base_df.columns]
            
            if missing_columns:
                raise ValueError(f"Ø£Ø¹Ù…Ø¯Ø© Ù…Ø·Ù„ÙˆØ¨Ø© Ù…ÙÙ‚ÙˆØ¯Ø©: {missing_columns}")
            
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            original_count = len(self.knowledge_base_df)
            self.knowledge_base_df = self.knowledge_base_df.dropna(subset=["text"])
            self.knowledge_base_df["text"] = self.knowledge_base_df["text"].str.strip()
            self.knowledge_base_df = self.knowledge_base_df[self.knowledge_base_df["text"] != ""]
            
            final_count = len(self.knowledge_base_df)
            logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©: {final_count} Ø³Ø¬Ù„ (Ù…Ù† Ø£ØµÙ„ {original_count})")
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©: {str(e)}")
            raise
    
    async def _load_embeddings(self):
        """ØªØ­Ù…ÙŠÙ„ Ø£Ùˆ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù€ embeddings"""
        try:
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ù„Ù embeddings
            embedding_files = list(self.embeddings_path.glob("*.npy"))
            
            if embedding_files:
                # ØªØ­Ù…ÙŠÙ„ embeddings Ù…ÙˆØ¬ÙˆØ¯Ø©
                embedding_file = embedding_files[0]
                logger.info(f"ðŸ“¥ ØªØ­Ù…ÙŠÙ„ embeddings Ù…Ù†: {embedding_file}")
                self.embeddings = np.load(embedding_file)
                
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯
                if len(self.embeddings) != len(self.knowledge_base_df):
                    logger.warning("âš ï¸ Ø¹Ø¯Ù… ØªØ·Ø§Ø¨Ù‚ Ø£Ø¨Ø¹Ø§Ø¯ embeddings Ù…Ø¹ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ Ø³ÙŠØªÙ… Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙˆÙ„ÙŠØ¯")
                    await self._generate_embeddings()
                else:
                    logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ embeddings: {self.embeddings.shape}")
            else:
                # ØªÙˆÙ„ÙŠØ¯ embeddings Ø¬Ø¯ÙŠØ¯Ø©
                logger.info("ðŸ”„ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ embeddingsØŒ Ø³ÙŠØªÙ… Ø§Ù„ØªÙˆÙ„ÙŠØ¯...")
                await self._generate_embeddings()
                
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ embeddings: {str(e)}")
            # Ù…Ø­Ø§ÙˆÙ„Ø© ØªÙˆÙ„ÙŠØ¯ embeddings Ø¬Ø¯ÙŠØ¯Ø©
            await self._generate_embeddings()
    
    async def _generate_embeddings(self):
        """ØªÙˆÙ„ÙŠØ¯ embeddings Ø¬Ø¯ÙŠØ¯Ø©"""
        try:
            logger.info("ðŸ”„ Ø¨Ø¯Ø¡ ØªÙˆÙ„ÙŠØ¯ embeddings...")
            
            texts = self.knowledge_base_df["text"].tolist()
            
            # ØªÙˆÙ„ÙŠØ¯ embeddings Ø¹Ù„Ù‰ Ø¯ÙØ¹Ø§Øª
            batch_size = 32
            all_embeddings = []
            
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                logger.info(f"ðŸ”„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯ÙØ¹Ø© {i//batch_size + 1}/{(len(texts) + batch_size - 1)//batch_size}")
                
                batch_embeddings = self.model.encode(
                    batch_texts,
                    convert_to_tensor=True,
                    show_progress_bar=False
                ).cpu().numpy()
                
                all_embeddings.append(batch_embeddings)
                
                # Ø¥Ø¹Ø·Ø§Ø¡ ÙØ±ØµØ© Ù„Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ø£Ø®Ø±Ù‰
                await asyncio.sleep(0.01)
            
            # Ø¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ embeddings
            self.embeddings = np.vstack(all_embeddings)
            
            # Ø­ÙØ¸ embeddings
            output_file = self.embeddings_path / "generated_embeddings.npy"
            np.save(output_file, self.embeddings)
            
            logger.info(f"âœ… ØªÙ… ØªÙˆÙ„ÙŠØ¯ ÙˆØ­ÙØ¸ embeddings: {self.embeddings.shape}")
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ embeddings: {str(e)}")
            raise
    
    async def search(self, query: str, top_k: int = 15) -> List[Dict[str, Any]]:
        """ØªÙ†ÙÙŠØ° Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ"""
        if not self._is_ready:
            raise RuntimeError("Ù…Ø­Ø±Ùƒ Ø§Ù„Ø¨Ø­Ø« ØºÙŠØ± Ø¬Ø§Ù‡Ø²")
        
        if not query.strip():
            raise ValueError("Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙØ§Ø±Øº")
        
        try:
            import time
            start_time = time.time()
            
            # ØªØ´ÙÙŠØ± Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
            query_embedding = self.model.encode([query], convert_to_tensor=True).cpu().numpy()
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
            similarities = cosine_similarity(query_embedding, self.embeddings).flatten()
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            top_indices = similarities.argsort()[-top_k:][::-1]
            
            # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            results = []
            for rank, idx in enumerate(top_indices, 1):
                record_id = self.knowledge_base_df.iloc[idx]["id"]
                record_text = self.knowledge_base_df.iloc[idx]["text"]
                score = float(similarities[idx])
                
                results.append({
                    "rank": rank,
                    "id": record_id,
                    "text": record_text,
                    "score": score
                })
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            processing_time = time.time() - start_time
            self.search_count += 1
            self.total_processing_time += processing_time
            
            logger.info(f"ðŸ” ØªÙ… Ø§Ù„Ø¨Ø­Ø« Ø¹Ù†: '{query}' ÙÙŠ {processing_time:.3f} Ø«Ø§Ù†ÙŠØ©")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {str(e)}")
            raise
    
    async def get_statistics(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…"""
        avg_processing_time = (
            self.total_processing_time / self.search_count 
            if self.search_count > 0 else 0
        )
        
        return {
            "total_searches": self.search_count,
            "average_processing_time": avg_processing_time,
            "knowledge_base_size": len(self.knowledge_base_df) if self.knowledge_base_df is not None else 0,
            "embeddings_shape": self.embeddings.shape if self.embeddings is not None else None,
            "model_loaded": self.model is not None,
            "system_ready": self._is_ready
        }
    
    async def get_model_info(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        info = {
            "model_path": str(self.model_path),
            "model_loaded": self.model is not None
        }
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ù‚Ø±Ø§Ø¡Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        try:
            training_info_file = self.model_path / "training_info.json"
            if training_info_file.exists():
                import json
                with open(training_info_file, 'r', encoding='utf-8') as f:
                    training_info = json.load(f)
                info["training_info"] = training_info
        except Exception as e:
            logger.warning(f"Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {str(e)}")
        
        return info
    
    def is_ready(self) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬Ø§Ù‡Ø²ÙŠØ© Ø§Ù„Ù†Ø¸Ø§Ù…"""
        return self._is_ready