
import os
import torch
from sentence_transformers import SentenceTransformer

class CPUOptimizedConfig:
    """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø­Ø³Ù†Ø© Ù„Ù„ØªØ´ØºÙŠÙ„ Ø¹Ù„Ù‰ CPU"""
    
    @staticmethod
    def setup_cpu_environment():
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ¦Ø© Ù„Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø­Ø³Ù† Ø¹Ù„Ù‰ CPU"""
        
        # ØªØ¹Ø·ÙŠÙ„ CUDA Ù†Ù‡Ø§Ø¦ÙŠØ§Ù‹
        os.environ["CUDA_VISIBLE_DEVICES"] = ""
        
        # ØªØ­Ø³ÙŠÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… CPU threads
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†ÙˆÙ‰ Ø§Ù„Ù…ØªØ§Ø­Ø©
        cpu_count = os.cpu_count()
        torch.set_num_threads(cpu_count)
        
        # ØªØ­Ø³ÙŠÙ† BLAS Ù„Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ©
        os.environ["OMP_NUM_THREADS"] = str(cpu_count)
        os.environ["MKL_NUM_THREADS"] = str(cpu_count)
        
        # ØªØ­Ø³ÙŠÙ† tokenizer
        os.environ["TOKENIZERS_PARALLELISM"] = "true"
        
        print(f"âœ… ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ´ØºÙŠÙ„ Ø¹Ù„Ù‰ CPU Ù…Ø¹ {cpu_count} Ù†ÙˆØ§Ø©")
    
    @staticmethod
    def get_optimal_batch_size():
        """Ø­Ø³Ø§Ø¨ Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„Ø£Ù…Ø«Ù„ Ø­Ø³Ø¨ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…ØªØ§Ø­Ø©"""
        try:
            import psutil
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…ØªØ§Ø­Ø© (GB)
            available_memory = psutil.virtual_memory().available / (1024**3)
            
            if available_memory >= 12:
                return 64  # Ø°Ø§ÙƒØ±Ø© ÙƒØ¨ÙŠØ±Ø©
            elif available_memory >= 8:
                return 32  # Ø°Ø§ÙƒØ±Ø© Ù…ØªÙˆØ³Ø·Ø©
            elif available_memory >= 4:
                return 16  # Ø°Ø§ÙƒØ±Ø© Ù‚Ù„ÙŠÙ„Ø©
            else:
                return 8   # Ø°Ø§ÙƒØ±Ø© Ù…Ø­Ø¯ÙˆØ¯Ø© Ø¬Ø¯Ø§Ù‹
                
        except ImportError:
            # Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† psutil Ù…ØªÙˆÙØ±Ø©ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ù‚ÙŠÙ…Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
            return 16
    
    @staticmethod
    def load_model_cpu_optimized(model_path: str):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª CPU"""
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ¦Ø©
        CPUOptimizedConfig.setup_cpu_environment()
        
        try:
            print("ğŸ”„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª CPU...")
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Ø¥Ø¬Ø¨Ø§Ø± Ø§Ø³ØªØ®Ø¯Ø§Ù… CPU
            model = SentenceTransformer(
                model_path,
                device='cpu'  # Ø¥Ø¬Ø¨Ø§Ø± Ø§Ø³ØªØ®Ø¯Ø§Ù… CPU
            )
            
            # ØªØ­Ø³ÙŠÙ†Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
            model.eval()  # ÙˆØ¶Ø¹ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡
            
            # ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
            # model.half()  # ØªÙ‚Ù„ÙŠÙ„ Ø¯Ù‚Ø© Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø© (Ù‚Ø¯ ÙŠØ¤Ø«Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ù‚Ø©)
            
            print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­ Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª CPU")
            return model
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")
            raise
    
    @staticmethod
    def get_cpu_info():
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬"""
        try:
            import psutil
            import platform
            
            cpu_info = {
                "processor": platform.processor(),
                "cores": psutil.cpu_count(logical=False),
                "threads": psutil.cpu_count(logical=True),
                "frequency": psutil.cpu_freq().current if psutil.cpu_freq() else "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ",
                "memory_total": round(psutil.virtual_memory().total / (1024**3), 2),
                "memory_available": round(psutil.virtual_memory().available / (1024**3), 2),
                "pytorch_threads": torch.get_num_threads()
            }
            
            return cpu_info
            
        except ImportError:
            return {"error": "psutil ØºÙŠØ± Ù…ØªÙˆÙØ± Ù„Ø¹Ø±Ø¶ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…"}


# ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª ÙÙŠ semantic_search.py
class CPUOptimizedSemanticSearch:
    """Ù…Ø­Ø±Ùƒ Ø¨Ø­Ø« Ù…Ø­Ø³Ù† Ù„Ù„Ù€ CPU"""
    
    def __init__(self, model_path: str, data_path: str, embeddings_path: str):
        self.model_path = model_path
        self.data_path = data_path
        self.embeddings_path = embeddings_path
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª
        self.cpu_config = CPUOptimizedConfig()
        self.batch_size = self.cpu_config.get_optimal_batch_size()
        
        print(f"ğŸ”§ Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„Ù…Ø­Ø³Ù†: {self.batch_size}")
        
        # Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
        self.model = None
        self.knowledge_base_df = None
        self.embeddings = None
        self._is_ready = False
    
    async def _load_model_optimized(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª CPU"""
        try:
            self.model = self.cpu_config.load_model_cpu_optimized(self.model_path)
            
            # Ø·Ø¨Ø§Ø¹Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
            cpu_info = self.cpu_config.get_cpu_info()
            print("ğŸ’» Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…:")
            for key, value in cpu_info.items():
                print(f"   {key}: {value}")
                
        except Exception as e:
            print(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ø³Ù†: {e}")
            # Ø§Ù„ØªØ±Ø§Ø¬Ø¹ Ù„Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©
            self.model = SentenceTransformer(self.model_path, device='cpu')
    
    async def _generate_embeddings_cpu_optimized(self):
        """ØªÙˆÙ„ÙŠØ¯ embeddings Ù…Ø­Ø³Ù† Ù„Ù„Ù€ CPU"""
        try:
            print("ğŸ”„ Ø¨Ø¯Ø¡ ØªÙˆÙ„ÙŠØ¯ embeddings Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª CPU...")
            
            texts = self.knowledge_base_df["text"].tolist()
            total_texts = len(texts)
            
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯ÙØ¹Ø§Øª ØµØºÙŠØ±Ø© Ù…Ø¹ progress bar
            all_embeddings = []
            batch_size = self.batch_size
            
            for i in range(0, total_texts, batch_size):
                batch_texts = texts[i:i + batch_size]
                progress = (i + len(batch_texts)) / total_texts * 100
                
                print(f"ğŸ”„ Ù…Ø¹Ø§Ù„Ø¬Ø©: {progress:.1f}% ({i + len(batch_texts)}/{total_texts})")
                
                # ØªÙˆÙ„ÙŠØ¯ embeddings Ù„Ù„Ø¯ÙØ¹Ø©
                batch_embeddings = self.model.encode(
                    batch_texts,
                    convert_to_tensor=False,  # Ø§Ø³ØªØ®Ø¯Ø§Ù… numpy Ù…Ø¨Ø§Ø´Ø±Ø©
                    device='cpu',
                    show_progress_bar=False,
                    batch_size=batch_size,
                    normalize_embeddings=True  # ØªØ·Ø¨ÙŠØ¹ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ø£ÙØ¶Ù„
                )
                
                all_embeddings.append(batch_embeddings)
                
                # Ø¥Ø¹Ø·Ø§Ø¡ ÙØ±ØµØ© Ù„Ù„Ù†Ø¸Ø§Ù… Ù„Ù„ØªÙ†ÙØ³
                await asyncio.sleep(0.1)
            
            # Ø¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ embeddings
            import numpy as np
            self.embeddings = np.vstack(all_embeddings)
            
            print(f"âœ… ØªÙ… ØªÙˆÙ„ÙŠØ¯ embeddings: {self.embeddings.shape}")
            
        except Exception as e:
            print(f"âŒ ÙØ´Ù„ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ embeddings: {e}")
            raise